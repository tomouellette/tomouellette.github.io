<!DOCTYPE html>
<html>
<head>

  
  <meta charset="utf-8">
  
  
  
  <title>Tom W. Ouellette | Inferring cancer evolution using synthetic supervised learning</title>
  
  <meta name="description" content="A personal website for Tom W. Ouellette">
  <meta name="author" content="Tom W. Ouellette">

  
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Crimson+Pro&display=swap" rel="stylesheet">

  
  <link rel="stylesheet" href="https://tomouellette.github.io/css/normalize.css">
  <link rel="stylesheet" href="https://tomouellette.github.io/css/skeleton.css">
  <link rel="stylesheet" href="https://tomouellette.github.io/css/animate.css/animate.min.css">

  
  <link rel="icon" type="image/png" href="https://tomouellette.github.io/images/favicon.png">

</head>
<body>
<div class="container">
      <div class="row">
        <div class="two-thirds column" style="margin-top: 10%">
          <h3 class="head-fade fadeInDown">Tom W. Ouellette</h3>
        </div>
      </div>
    </div>

<div class="body-fade fadeInDown">
  <div class="container">
    <div class="row">
        <div class="twelve columns" style="margin-top: 0%">
          <nav id="nav" class="nav">
          
          <a class="button nav-but" href="/">
           
           about
          </a> 
          
          <a class="button nav-but" href="/projects&#43;software/">
           
           projects & software
          </a> 
          
          <a class="button nav-but" href="/thoughts/">
           
           thoughts
          </a> 
          
        </nav>
      </div>
    </div>
  </div>
</div>

<div id="content">



<div class="body-fade fadeInDown">
<div class="container">
  <div class="twelve columns intro" style="margin-top: 2%">
	<div id = "about">
	

<time datetime="2021-11-26">Nov 26, 2021</time>



	<h3>Inferring cancer evolution using synthetic supervised learning</h3>		
	
	<div style="font-size:12px; font-family:'Helvetica', sans-serif;"><p>Our paper <a href="https://www.biorxiv.org/content/10.1101/2021.11.22.469566v1">Inferring ongoing cancer evolution from single tumour biopsies using synthetic supervised learning</a> tries to lay some groundwork for integrating simulation, or synthetic data, based methods with deep learning for making predictions on cancer evolutionary parameters (like the number of subclones, subclone frequency, positive selection) using allele/mutation frequency information. I give a brief synopsis of the paper below but if you&rsquo;re interested in just playing around with interactive plots, downloading models, or playing around with code here are the quick links.</p>
<h5 id="quick-links">Quick links</h5>
<ul>
<li><a href="https://www.biorxiv.org/content/10.1101/2021.11.22.469566v1">Preprint</a> (bioRxiv)</li>
<li><a href="https://tomouellette.gitlab.io/ouellette_awadalla_2021/index.html">Supplementary</a> (HTML)</li>
<li><a href="https://github.com/tomouellette/CanEvolve.jl">Synthetic data generation</a> (Github)</li>
<li><a href="https://github.com/tomouellette/TumE">Synthetic supervised learning</a> (Github)</li>
</ul>
<h5 id="overview">Overview</h5>
<p>In the paper, we emphasize four points. <i>Firstly</i>, simulation-based methods are more flexible, and better connected, in regards to explicit models of evolution. Practically, this means we can account for neutral/non-neutral dynamics and you can make inferences using any stochastic generative process/simulator/synthetic dataset, regardless of the likelihood tractability. <i>Secondly</i>, by using neural networks to solve the inverse problem of inferring parameters from the data we can take advantage of the full dimensionality of the VAF distribution. This avoids single statistics and distance metrics that compress the data, sometimes poorly, prior to inference. <i>Thirdly</i>, separating simulation from model training allows for fast inference through amortization - this differs from other forms of likelihood-free inference where millions of simulations must be generated for inference in each individual sample. Inference per sample, with synthetic supervised learning, is on the scale of 1 second, compared to minutes for mixture models and hours for approximate Bayesian computation. <i>Lastly</i>, we highlight how we can use transfer learning to develop new models for related but different evolutionary inference tasks.</p>
<h5 id="generating-synthetic-data">Generating synthetic data</h5>
<p>We put together <a href="https://github.com/tomouellette/CanEvolve.jl">CanEvolve.jl</a> to automate generation of synthetic tumour sequencing data under neutral evolution and positive selection. It utilizes the rKMC algorithm developed by Williams et al. (<a href="https://github.com/marcjwilliams1/CancerSeqSim.jl">CancerSeqSim.jl</a>) to generate synthetic data under a stochastic branching process of exponential growth. However, rather than deterministically initiate subclones we allow for the random arrival of driver mutations that multiplicatively increase fitness over time. We also integrate a generative sampling process to quickly &lsquo;simulate&rsquo; frequency distributions that recapitulate neutral evolution. Here&rsquo;s an example of paired sequencing data with and without selected subclonal populations.</p>
<img src="/tume_fig1.gif" style="width: 100%">
<h5 id="synthetic-supervised-learning">Synthetic supervised learning</h5>
<p>Given synthetic sequencing data generated under reasonable parameter regimes/priors, we can train deep learning models to make inferences on evolutionary parameters. <a href="https://github.com/tomouellette/TumE">TumE</a> is a synthetic supervised learning method for inferring positive selection, the number of selected subclones, and subclone frequencies from bulk-sequenced, single tumour biopsies. We provide hundreds of pre-trained models that anyone can take advantage of (see our <a href="https://www.doi.org/10.5281/zenodo.5575877">Zenodo</a> repository). We also built a set of models using transfer learning (open set domain adaptation) to infer additional parameters such as subclone fitness, subclone emergence time, and mutation rate using an alternative simulation framework <a href="https://github.com/T-Heide/TEMULATOR">TEMULATOR</a>. Here&rsquo;s an example of inference in a high-depth, bulk sequenced acute myeloid leukemia sample.</p>
<center><img src="/tume_fig2.png" style="width: 60%;"></center>
<br>
<h5 id="limitations-of-synthetic-supervised-learning-approach">Limitations of synthetic supervised learning approach</h5>
<p>Probably the most obvious limitation is that we are working with a non-linear parametric function i.e. a neural network. This means our predictions are limited to the constraints we place on our synthetic dataset. In this case, we only can make estimates on up to 2 subclones (in addition to the neutral tail and clonal peak) in a 1-dimensional VAF distribution. I personally don&rsquo;t think this is a limitation since it&rsquo;s pretty unlikely to detect even 2 subclones in noisy VAF distributions. However, if you move beyond only considering diploid regions there is potential that more subclonal heterogeneity exists. Scaling this method to arbitrary ploidy may be something fun to play around with. One benefit that hope gets further explored is how transfer learning can be used to speed up these changes in model assumptions. Working on optimizing this workflow in the future.</p>
<h5 id="some-forward-looking-thoughts-on-simulation-based-inference">Some forward-looking thoughts on simulation-based inference</h5>
<p>Looking to the future, there is a lot of really cool avenues this could be expanded too. I personally feel like transfer learning can make a big impact in the simulation/likelihood-free inference world. Model sharing is much easier when all the information is compressed into a set of weights, especially when simulations are expensive. More creative conditioning of models w.r.t different assumptions may facilitate model selection, e.g. logistic growth versus exponential growth. And switching to deep generative models, rather than vanilla deterministic networks, might be better suited for interpretability. Recent work using neural density estimation as a replacement for ABC has seen a lot of development, but it&rsquo;s not entirely clear if the methods scale to expensive simulations like the ones used in cancer evolution (check out Conor Durkan&rsquo;s <a href="https://github.com/conormdurkan/lfi/">LFI repo</a>). Looking forward with simulation-based inference, broader application likely needs faster, computationally cheaper simulations (emulators, maybe?) and/or better scaling of existing likelihood-free approaches (driven by better proposals every iteration?). From what I&rsquo;ve seen, a lot of interesting work along these lines is ongoing in the <a href="https://www.mackelab.org/">Macke lab</a>. Looking forward to seeing how things evolve.</p>
</div>
	</div>
  </div>
</div>
</div>


        </div><p class="footer text-center"></p>
</body>
</html>
